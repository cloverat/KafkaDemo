# ------------------------------------------ consumer ------------------------------------------
## kafka服务器配置，kafka多节点时候使用,逗号隔开
kafka.consumer.bootstrap-servers=127.0.0.1:9092

# 消费者监听的topic
kafka.consumer.topic.xxx=producer_topic
# kafka消费组id，不同组名可以重复消费。例如你先使用了组名A消费了kafka的1000条数据，但是你还想再次进行消费这1000条数据，并且不想重新去产生，那么这里你只需要更改组名就可以重复消费了
kafka.consumer.group-id=consumer_group_test

# earliest: 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
# latest: （默认）当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
# none: topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
# 首次上线使用latest，后续修改为earliest（如果首次就用earliest，该主题下所有消息会重新推送一遍，但是如果用latest，会以启动完成的节点为起点开始，中间的拜访会丢失）
kafka.consumer.auto-offset-reset=latest
# 是否自动提交offset，默认为true（告知kafka当前consumer group读取到的消息位置）
kafka.consumer.enable-auto-commit=true
# 偏移量自动提交到Kafka的频率（以毫秒为单位），默认5000
kafka.consumer.auto-commit-interval-ms=1000
kafka.consumer.key.serializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.consumer.value.serializer=org.apache.kafka.common.serialization.StringDeserializer

# 超时时间，默认30000
kafka.consumer.session-timeout-ms=30000
kafka.consumer.poll-timeout=1000

# ------------------------------------------ producer ------------------------------------------

## kafka服务器配置，kafka多节点时候使用,逗号隔开
kafka.producer.bootstrap-servers=127.0.0.1:9092
# 生产者发送的topic
kafka.producer.topic.xxx=producer_topic
kafka.producer.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.producer.value.serializer=org.apache.kafka.common.serialization.StringSerializer

